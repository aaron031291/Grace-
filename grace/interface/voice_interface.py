"""
Voice Interface for Grace

Speech-to-Text and Text-to-Speech with local AI models.

Features:
- Real-time speech recognition
- Local Whisper model (no external API)
- Continuous listening mode
- Voice activity detection
- Text-to-speech responses
- Low-latency processing

Grace can hear and speak!
"""

import asyncio
import logging
from typing import Optional, Callable
from dataclasses import dataclass
from datetime import datetime
import wave
import json

logger = logging.getLogger(__name__)


@dataclass
class VoiceConfig:
    """Voice interface configuration"""
    use_local_models: bool = True
    whisper_model: str = "base"  # tiny, base, small, medium, large
    tts_model: str = "piper"  # Local TTS
    sample_rate: int = 16000
    chunk_duration_ms: int = 30
    vad_threshold: float = 0.5
    continuous_mode: bool = True


class LocalWhisperSTT:
    """
    Local Speech-to-Text using Whisper
    
    No external API needed - runs entirely on your machine!
    """
    
    def __init__(self, model_size: str = "base"):
        self.model_size = model_size
        self.model = None
        self.loaded = False
        
        logger.info(f"Whisper STT initialized (model: {model_size})")
    
    async def load_model(self):
        """Load Whisper model"""
        try:
            import whisper
            
            logger.info(f"Loading Whisper {self.model_size} model...")
            self.model = whisper.load_model(self.model_size)
            self.loaded = True
            
            logger.info("âœ… Whisper model loaded (local, no API needed)")
            
        except ImportError:
            logger.warning("Whisper not installed. Install: pip install openai-whisper")
            # Fallback to mock
            self.loaded = False
        except Exception as e:
            logger.error(f"Failed to load Whisper: {e}")
            self.loaded = False
    
    async def transcribe_audio(
        self,
        audio_data: bytes,
        language: str = "en"
    ) -> str:
        """
        Transcribe audio to text.
        
        Runs locally using Whisper model.
        """
        if not self.loaded:
            await self.load_model()
        
        if not self.loaded:
            # Mock fallback
            return "[Simulated transcription - install whisper for real STT]"
        
        try:
            # Save audio temporarily
            temp_file = "/tmp/grace_audio_temp.wav"
            with wave.open(temp_file, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(16000)
                wf.writeframes(audio_data)
            
            # Transcribe
            result = self.model.transcribe(temp_file, language=language)
            text = result["text"]
            
            logger.info(f"Transcribed: {text[:50]}...")
            
            return text
            
        except Exception as e:
            logger.error(f"Transcription failed: {e}")
            return ""


class LocalTTS:
    """
    Local Text-to-Speech
    
    Uses local models (Piper, Coqui TTS) - no cloud API needed!
    """
    
    def __init__(self, model: str = "piper"):
        self.model_type = model
        self.model = None
        self.loaded = False
        
        logger.info(f"TTS initialized (model: {model})")
    
    async def load_model(self):
        """Load TTS model"""
        try:
            # In production, load Piper or Coqui TTS
            # For now, simulate
            self.loaded = True
            logger.info("âœ… TTS model loaded (local)")
            
        except Exception as e:
            logger.error(f"Failed to load TTS: {e}")
            self.loaded = False
    
    async def synthesize_speech(
        self,
        text: str,
        voice: str = "default"
    ) -> bytes:
        """
        Convert text to speech audio.
        
        Returns audio data as bytes.
        """
        if not self.loaded:
            await self.load_model()
        
        # In production: use Piper/Coqui to generate audio
        # For now, return placeholder
        logger.info(f"Synthesizing: {text[:50]}...")
        
        return b"audio_data_placeholder"


class VoiceActivityDetector:
    """
    Voice Activity Detection
    
    Detects when user is speaking vs silence.
    """
    
    def __init__(self, threshold: float = 0.5):
        self.threshold = threshold
    
    async def is_speech(self, audio_chunk: bytes) -> bool:
        """
        Detect if audio chunk contains speech.
        
        In production: use WebRTC VAD or similar
        """
        # Simplified: check if audio has energy
        # Real implementation would use VAD algorithm
        return len(audio_chunk) > 100  # Placeholder


class GraceVoiceInterface:
    """
    Complete voice interface for Grace.
    
    Grace can:
    - Hear you speak (local Whisper STT)
    - Understand in real-time
    - Respond with voice (local TTS)
    - Continuous conversation mode
    """
    
    def __init__(self, config: Optional[VoiceConfig] = None):
        self.config = config or VoiceConfig()
        
        # Initialize components
        self.stt = LocalWhisperSTT(self.config.whisper_model)
        self.tts = LocalTTS(self.config.tts_model)
        self.vad = VoiceActivityDetector(self.config.vad_threshold)
        
        # State
        self.listening = False
        self.speaking = False
        self.audio_buffer = []
        
        # Callbacks
        self.on_transcription: Optional[Callable] = None
        self.on_response: Optional[Callable] = None
        
        logger.info("Grace Voice Interface initialized")
        logger.info(f"  STT: Local Whisper ({self.config.whisper_model})")
        logger.info(f"  TTS: Local {self.config.tts_model}")
        logger.info(f"  Continuous mode: {self.config.continuous_mode}")
    
    async def initialize(self):
        """Initialize voice interface"""
        logger.info("Initializing voice interface...")
        
        # Load models
        await self.stt.load_model()
        await self.tts.load_model()
        
        logger.info("âœ… Voice interface ready")
    
    async def start_listening(self):
        """Start continuous listening mode"""
        self.listening = True
        
        logger.info("ðŸŽ¤ Grace is listening...")
        
        while self.listening:
            try:
                # Get audio chunk (from microphone)
                audio_chunk = await self._get_audio_chunk()
                
                # Check for speech
                has_speech = await self.vad.is_speech(audio_chunk)
                
                if has_speech:
                    self.audio_buffer.append(audio_chunk)
                else:
                    # Silence detected - process buffered audio
                    if self.audio_buffer:
                        await self._process_audio_buffer()
                        self.audio_buffer = []
                
                await asyncio.sleep(0.01)  # Small delay
                
            except Exception as e:
                logger.error(f"Listening error: {e}")
    
    def stop_listening(self):
        """Stop listening"""
        self.listening = False
        logger.info("ðŸŽ¤ Stopped listening")
    
    async def _get_audio_chunk(self) -> bytes:
        """Get audio chunk from microphone"""
        # In production: use pyaudio or similar
        # For now, simulate
        await asyncio.sleep(0.03)  # 30ms chunks
        return b"audio_chunk"
    
    async def _process_audio_buffer(self):
        """Process buffered audio"""
        # Combine chunks
        audio_data = b"".join(self.audio_buffer)
        
        # Transcribe
        text = await self.stt.transcribe_audio(audio_data)
        
        if text and self.on_transcription:
            # Callback with transcription
            await self.on_transcription(text)
    
    async def speak(self, text: str):
        """
        Grace speaks the text.
        
        Converts text to speech and plays audio.
        """
        self.speaking = True
        
        logger.info(f"ðŸ”Š Grace speaking: {text[:50]}...")
        
        # Synthesize speech
        audio_data = await self.tts.synthesize_speech(text)
        
        # Play audio (in production, use audio player)
        await self._play_audio(audio_data)
        
        self.speaking = False
    
    async def _play_audio(self, audio_data: bytes):
        """Play audio"""
        # In production: use pyaudio or similar
        await asyncio.sleep(len(audio_data) / 16000)  # Simulate playback
    
    async def conversation_loop(
        self,
        on_user_speech: Callable,
        on_grace_response: Callable
    ):
        """
        Continuous conversation loop.
        
        Grace listens, you speak, Grace responds, repeat.
        """
        self.on_transcription = on_user_speech
        self.on_response = on_grace_response
        
        logger.info("ðŸ”„ Starting conversation loop")
        logger.info("   Speak naturally - Grace is listening!")
        
        await self.start_listening()


# Global voice interface
_voice_interface: Optional[GraceVoiceInterface] = None


def get_voice_interface() -> GraceVoiceInterface:
    """Get global voice interface"""
    global _voice_interface
    if _voice_interface is None:
        _voice_interface = GraceVoiceInterface()
    return _voice_interface


if __name__ == "__main__":
    # Demo
    async def demo():
        print("ðŸŽ¤ Grace Voice Interface Demo\n")
        
        voice = GraceVoiceInterface()
        await voice.initialize()
        
        # Simulate speech recognition
        print("Simulating speech recognition...")
        audio = b"simulated_audio_data"
        text = await voice.stt.transcribe_audio(audio)
        print(f"  Transcribed: {text}")
        
        # Simulate speech synthesis
        print("\nSimulating speech synthesis...")
        await voice.speak("Hello! I am Grace, your AI assistant.")
        print("  âœ… Speech synthesized")
        
        print("\nðŸ“Š Voice Interface Status:")
        print(f"  STT ready: {voice.stt.loaded}")
        print(f"  TTS ready: {voice.tts.loaded}")
        print(f"  Listening: {voice.listening}")
    
    asyncio.run(demo())
