#!/usr/bin/env python3
"""
Comprehensive test suite for Grace enhanced memory infrastructure and ML/DL specialists.

Tests:
- Vector database integration (Milvus/Pinecone simulation)
- Quantum-safe storage functionality
- Enhanced memory bridge operations
- New ML/DL specialists
- Cross-domain validation
- Enhanced governance liaison
"""

import asyncio
import pytest
import sys
import os
import tempfile
import json
from typing import Dict, List, Any
from datetime import datetime
import logging

# Add grace to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

import pytest


pytestmark = pytest.mark.e2e

async def test_vector_database_integration():
    """Test vector database functionality."""
    print("\nüß™ Testing Vector Database Integration...")

    try:
        from grace.memory.vector_db import (
            VectorMemoryCore,
            MilvusVectorDB,
            EmbeddingService,
        )

        # Test with mock Milvus (will fall back to simulated mode)
        vector_db = MilvusVectorDB(host="localhost", port=19530)
        embedding_service = EmbeddingService("all-MiniLM-L6-v2")

        # Test embedding service initialization (simulated)
        initialized = embedding_service.initialize()
        if not initialized:
            print("‚ö†Ô∏è  Embedding service not available, using simulation")
            # Simulate embedding service
            embedding_service.dimension = 384
            embedding_service.model = "simulated"

            def mock_encode(text):
                import hashlib
                import numpy as np

                # Generate deterministic "embeddings" from text hash
                hash_obj = hashlib.md5(str(text).encode())
                seed = int(hash_obj.hexdigest()[:8], 16)
                np.random.seed(seed)
                return np.random.rand(384).tolist()

            embedding_service.encode_text = mock_encode

        # Create vector memory core
        vector_memory = VectorMemoryCore(
            vector_db=vector_db,
            embedding_service=embedding_service,
            default_collection="test_collection",
        )

        # Test initialization (will simulate if real DB not available)
        await vector_db.connect()  # This will likely fail but handled gracefully

        # Test content storage (simulated)
        test_content = (
            "This is a test governance decision about AI ethics and transparency."
        )
        content_id = await vector_memory.store_content(
            content=test_content,
            content_id="test_content_1",
            metadata={"type": "governance_decision", "domain": "ethics"},
            trust_score=0.8,
            constitutional_score=0.9,
        )

        if content_id:
            print(f"‚úÖ Vector content stored: {content_id}")
        else:
            print("‚ö†Ô∏è  Vector storage simulated (DB not available)")

        # Test semantic search (simulated)
        search_results = await vector_memory.semantic_search(
            query="AI ethics governance", top_k=5, min_trust_score=0.5
        )

        print(f"‚úÖ Vector semantic search completed: {len(search_results)} results")

        # Test stats
        stats = vector_memory.get_stats()
        print(
            f"‚úÖ Vector database stats: {stats['search_count']} searches, {stats['insert_count']} inserts"
        )

        await vector_memory.shutdown()
        return True

    except ImportError as e:
        print(f"‚ö†Ô∏è  Vector database dependencies not available: {e}")
        return  # Consider this a success for CI environments
    except Exception as e:
        print(f"‚ùå Vector database test failed: {e}")
        assert False, f"Vector database test failed: {e}"


async def test_quantum_safe_storage():
    """Test quantum-safe storage functionality."""
    print("\nüß™ Testing Quantum-Safe Storage...")

    try:
        from grace.memory.quantum_safe_storage import (
            QuantumSafeStorageLayer,
            QuantumSafeKeyManager,
        )

        # Create temporary storage directory
        with tempfile.TemporaryDirectory() as temp_dir:
            storage_path = os.path.join(temp_dir, "quantum_storage")
            key_store_path = os.path.join(temp_dir, "quantum_keys")

            # Initialize quantum-safe storage
            key_manager = QuantumSafeKeyManager(key_store_path=key_store_path)
            storage_layer = QuantumSafeStorageLayer(
                storage_path=storage_path, key_manager=key_manager
            )

            # Test initialization
            initialized = await storage_layer.initialize()
            if not initialized:
                raise Exception("Failed to initialize quantum-safe storage")

            print("‚úÖ Quantum-safe storage initialized")

            # Test data storage
            test_data = {
                "decision_id": "test_decision_001",
                "outcome": "approved",
                "reasoning": "Constitutional compliance verified",
                "trust_score": 0.85,
                "sensitive_info": "classified governance data",
            }

            store_result = await storage_layer.store_encrypted(
                data=test_data,
                storage_id="test_decision_001",
                key_id="grace_governance",
                metadata={"classification": "sensitive", "domain": "governance"},
            )

            if store_result["status"] == "success":
                print(f"‚úÖ Quantum-safe data stored: {store_result['storage_id']}")
            else:
                raise Exception(f"Storage failed: {store_result}")

            # Test data retrieval
            retrieve_result = await storage_layer.retrieve_encrypted(
                storage_id="test_decision_001", return_format="json"
            )

            if retrieve_result["status"] == "success":
                retrieved_data = retrieve_result["data"]
                if retrieved_data["decision_id"] == test_data["decision_id"]:
                    print("‚úÖ Quantum-safe data retrieved and verified")
                else:
                    raise Exception("Data integrity check failed")
            else:
                raise Exception(f"Retrieval failed: {retrieve_result}")

            # Test key rotation
            rotation_result = await storage_layer.rotate_storage_keys()
            if rotation_result["status"] == "success":
                print(
                    f"‚úÖ Storage keys rotated: {rotation_result['keys_rotated']} keys"
                )

            # Test health check
            health_result = await storage_layer.health_check()
            if health_result["healthy"]:
                print("‚úÖ Quantum-safe storage health check passed")
            else:
                raise Exception(f"Health check failed: {health_result}")

            # Test secure deletion
            delete_result = await storage_layer.delete_encrypted("test_decision_001")
            if delete_result["status"] == "success":
                print("‚úÖ Quantum-safe secure deletion completed")

            # Get statistics
            stats = storage_layer.get_stats()
            print(
                f"‚úÖ Quantum-safe storage stats: {stats['write_count']} writes, {stats['read_count']} reads"
            )

            return

    except ImportError as e:
        print(f"‚ö†Ô∏è  Quantum-safe storage dependencies not available: {e}")
        return  # Consider this a success for CI environments
    except Exception as e:
        print(f"‚ùå Quantum-safe storage test failed: {e}")
        assert False, f"Quantum-safe storage test failed: {e}"


async def test_enhanced_memory_bridge():
    """Test enhanced memory bridge functionality."""
    print("\nüß™ Testing Enhanced Memory Bridge...")

    try:
        from grace.memory.enhanced_memory_bridge import EnhancedMemoryBridge
        from grace.core.memory_core import MemoryCore

        # Create enhanced memory bridge with minimal config
        config = {
            "vector_db": {"enabled": False},  # Disable vector DB for test
            "quantum_storage": {
                "enabled": True,
                "storage_path": "/tmp/test_quantum_storage",
            },
            "routing": {
                "governance_to_quantum": True,
                "unstructured_to_vector": False,  # Disabled
                "structured_to_traditional": True,
            },
        }

        bridge = EnhancedMemoryBridge(config=config)

        # Test initialization
        initialized = await bridge.initialize()
        if initialized:
            print("‚úÖ Enhanced memory bridge initialized")
        else:
            print("‚ö†Ô∏è  Enhanced memory bridge partially initialized")

        # Test enhanced storage
        test_content = {
            "type": "governance_decision",
            "decision": "Approve AI system deployment",
            "rationale": "Meets constitutional requirements",
            "stakeholders": ["ethics_board", "technical_review", "legal_counsel"],
        }

        store_result = await bridge.store_enhanced(
            content=test_content,
            content_type="governance_decision",
            metadata={"priority": "high", "classification": "sensitive"},
            trust_score=0.9,
            constitutional_score=0.85,
            sensitivity_score=0.8,
        )

        if store_result["status"] == "success":
            print(f"‚úÖ Enhanced storage completed: {store_result['content_id']}")
            stored_content_id = store_result["content_id"]
        else:
            print(f"‚ö†Ô∏è  Enhanced storage had issues: {store_result}")
            stored_content_id = None

        # Test enhanced recall
        if stored_content_id:
            recall_result = await bridge.recall_enhanced(
                content_id=stored_content_id,
                search_strategy="auto",
                include_similar=False,
            )

            if recall_result.get("found"):
                print("‚úÖ Enhanced recall successful")
            else:
                print("‚ö†Ô∏è  Enhanced recall failed")

        # Test enhanced search
        search_result = await bridge.search_enhanced(
            query="governance decision approval", search_type="hybrid", top_k=5
        )

        if search_result["status"] == "success":
            print(
                f"‚úÖ Enhanced search completed: {search_result['total_results']} results"
            )
        else:
            print(f"‚ö†Ô∏è  Enhanced search had issues: {search_result}")

        # Test health check
        health_result = await bridge.health_check()
        if health_result["overall_healthy"]:
            print("‚úÖ Enhanced memory bridge health check passed")
        else:
            print("‚ö†Ô∏è  Enhanced memory bridge health check failed")

        # Get statistics
        stats = bridge.get_stats()
        print(f"‚úÖ Enhanced memory bridge stats: {stats['total_requests']} requests")

        await bridge.shutdown()
        return True

    except Exception as e:
        print(f"‚ùå Enhanced memory bridge test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_enhanced_ml_specialists():
    """Test enhanced ML/DL specialists."""
    print("\nüß™ Testing Enhanced ML/DL Specialists...")

    try:
        from grace.mldl.specialists.enhanced_specialists import (
            GraphNeuralNetworkSpecialist,
            MultimodalAISpecialist,
            UncertaintyQuantificationSpecialist,
        )

        # Test Graph Neural Network Specialist
        gnn_specialist = GraphNeuralNetworkSpecialist()
        await gnn_specialist.initialize()

        # Test relationship analysis
        gnn_data = {
            "task": "relationship_analysis",
            "entities": [
                {"id": "ethics_board", "type": "committee"},
                {"id": "technical_team", "type": "team"},
                {"id": "ai_system", "type": "system"},
            ],
            "relationships": [
                {
                    "source": "ethics_board",
                    "target": "ai_system",
                    "type": "oversight",
                    "strength": 0.8,
                },
                {
                    "source": "technical_team",
                    "target": "ai_system",
                    "type": "development",
                    "strength": 0.9,
                },
            ],
        }

        gnn_prediction = await gnn_specialist.predict_with_uncertainty(gnn_data)
        if gnn_prediction.confidence > 0.0:
            print(
                f"‚úÖ GNN specialist prediction: {gnn_prediction.confidence:.2f} confidence"
            )
        else:
            print("‚ö†Ô∏è  GNN specialist had issues")

        # Test Multimodal AI Specialist
        multimodal_specialist = MultimodalAISpecialist()
        await multimodal_specialist.initialize()

        multimodal_data = {
            "task": "content_analysis",
            "content_types": ["text", "image"],
            "content": {
                "text": "AI governance framework for constitutional compliance",
                "image": "governance_flowchart.png",
            },
        }

        multimodal_prediction = await multimodal_specialist.predict_with_uncertainty(
            multimodal_data
        )
        if multimodal_prediction.confidence > 0.0:
            print(
                f"‚úÖ Multimodal specialist prediction: {multimodal_prediction.confidence:.2f} confidence"
            )
        else:
            print("‚ö†Ô∏è  Multimodal specialist had issues")

        # Test Uncertainty Quantification Specialist
        uncertainty_specialist = UncertaintyQuantificationSpecialist()
        await uncertainty_specialist.initialize()

        uncertainty_data = {
            "task": "uncertainty_analysis",
            "predictions": [0.8, 0.7, 0.9, 0.75, 0.82],
            "model_confidence": [0.85, 0.72, 0.88, 0.78, 0.80],
        }

        uncertainty_prediction = await uncertainty_specialist.predict_with_uncertainty(
            uncertainty_data
        )
        if uncertainty_prediction.confidence > 0.0:
            print(
                f"‚úÖ Uncertainty specialist prediction: {uncertainty_prediction.uncertainty:.2f} uncertainty"
            )
        else:
            print("‚ö†Ô∏è  Uncertainty specialist had issues")

        # Test specialist statistics
        gnn_stats = gnn_specialist.get_stats()
        print(f"‚úÖ Specialist stats - GNN trust score: {gnn_stats['trust_score']:.2f}")

        return

    except Exception as e:
        print(f"‚ùå Enhanced ML specialists test failed: {e}")
        import traceback

        traceback.print_exc()
        assert False, f"Enhanced ML specialists test failed: {e}"


async def test_cross_domain_validators():
    """Test cross-domain validators."""
    print("\nüß™ Testing Cross-Domain Validators...")

    try:
        from grace.mldl.specialists.enhanced_specialists import (
            CrossDomainValidator,
            SpecialistPrediction,
            create_cross_domain_validators,
        )

        # Create validators
        validators = create_cross_domain_validators()
        print(f"‚úÖ Created {len(validators)} cross-domain validators")

        # Test data for validation
        decision_data = {
            "decision_type": "ai_deployment_approval",
            "technical_feasibility": 0.8,
            "legal_compliance": 0.9,
            "fairness_score": 0.7,
            "security_risks": {"data_breach": 0.2, "adversarial_attack": 0.3},
            "bias_indicators": {"gender": 0.1, "age": 0.15},
            "compliance_factors": {
                "fairness": 0.8,
                "transparency": 0.9,
                "accountability": 0.85,
                "regulatory": 0.9,
                "constitutional": 0.88,
            },
        }

        # Create mock specialist predictions
        mock_predictions = [
            SpecialistPrediction(
                prediction={"approval": True, "confidence": 0.8},
                confidence=0.8,
                uncertainty=0.2,
                explanation="Technical analysis supports approval",
                evidence=[
                    {"type": "technical_analysis", "quality": 0.9, "consistency": 0.8}
                ],
                cross_domain_score=0.75,
                hallucination_risk=0.1,
            ),
            SpecialistPrediction(
                prediction={"ethical_score": 0.85},
                confidence=0.85,
                uncertainty=0.15,
                explanation="Ethical review shows compliance",
                evidence=[
                    {"type": "ethical_analysis", "quality": 0.8, "consistency": 0.9}
                ],
                cross_domain_score=0.8,
                hallucination_risk=0.2,
            ),
        ]

        # Test each validator
        validation_results = {}
        for validator in validators:
            try:
                validation = await validator.validate_decision(
                    decision_data, mock_predictions, context={"urgent": False}
                )

                validation_results[validator.domain] = validation
                print(
                    f"‚úÖ {validator.domain} validation: {validation.validation_score:.2f} score"
                )

                if validation.red_flags:
                    print(
                        f"üö© {validator.domain} red flags: {', '.join(validation.red_flags)}"
                    )

            except Exception as e:
                print(f"‚ö†Ô∏è  {validator.domain} validation failed: {e}")

        # Test validator statistics
        for validator in validators:
            stats = validator.get_stats()
            print(
                f"‚úÖ {validator.domain} validator stats: {stats['total_validations']} validations"
            )

        return len(validation_results) > 0

    except Exception as e:
        print(f"‚ùå Cross-domain validators test failed: {e}")
        import traceback

        traceback.print_exc()
        assert False, f"Cross-domain validators test failed: {e}"


async def test_enhanced_governance_liaison():
    """Test enhanced governance liaison."""
    print("\nüß™ Testing Enhanced Governance Liaison...")

    try:
        from grace.mldl.enhanced_governance_liaison import EnhancedGovernanceLiaison

        # Create enhanced governance liaison
        liaison = EnhancedGovernanceLiaison()

        # Test initialization
        initialized = await liaison.initialize()
        if initialized:
            print("‚úÖ Enhanced governance liaison initialized")
        else:
            print("‚ö†Ô∏è  Enhanced governance liaison initialization had issues")

        # Test governance request processing
        test_request = {
            "request_id": "test_governance_001",
            "task_type": "ai_deployment_decision",
            "data": {
                "system_name": "AI Ethics Advisor",
                "deployment_scope": "governance_support",
                "risk_level": "medium",
            },
            "context": {
                "urgency": "normal",
                "stakeholders": ["ethics_board", "technical_team", "legal_counsel"],
            },
            "entities": [
                {"id": "ai_system", "type": "software"},
                {"id": "users", "type": "stakeholder"},
            ],
            "relationships": [
                {
                    "source": "ai_system",
                    "target": "users",
                    "type": "serves",
                    "strength": 0.9,
                }
            ],
            "content": {
                "text": "Request for approval of AI system deployment in governance context"
            },
            "content_types": ["text"],
            "predictions": [0.8, 0.75, 0.85],
            "model_confidence": [0.82, 0.78, 0.88],
            "technical_feasibility": 0.9,
            "legal_compliance": 0.85,
            "fairness_score": 0.8,
            "security_risks": {"data_breach": 0.1, "bias": 0.2},
            "compliance_factors": {
                "fairness": 0.8,
                "transparency": 0.9,
                "accountability": 0.85,
            },
        }

        # Process governance request
        governance_result = await liaison.process_governance_request(test_request)

        if governance_result["status"] == "success":
            print(f"‚úÖ Governance processing successful:")
            print(
                f"   - Specialist predictions: {governance_result['specialist_predictions']}"
            )
            print(
                f"   - Consensus reached: {governance_result['consensus']['consensus_reached']}"
            )
            print(
                f"   - Validation passed: {governance_result['validation']['validation_passed']}"
            )
            print(
                f"   - Hallucination risk: {governance_result['hallucination_risk']:.2f}"
            )
            print(
                f"   - Decision: {governance_result['governance_decision']['decision_type']}"
            )
        else:
            print(f"‚ö†Ô∏è  Governance processing had issues: {governance_result}")

        # Test performance update (simulate)
        if governance_result["status"] == "success":
            update_result = await liaison.update_specialist_performance(
                governance_result["request_id"], actual_outcome="approved"
            )
            if update_result["status"] == "success":
                print(
                    f"‚úÖ Specialist performance updated: {len(update_result['updates'])} specialists"
                )

        # Get statistics
        stats = liaison.get_stats()
        print(f"‚úÖ Enhanced governance liaison stats:")
        print(f"   - Total requests: {stats['total_requests']}")
        print(f"   - Success rate: {stats['success_rate']:.2f}")
        print(
            f"   - Working specialists: {len([s for s in stats['specialists'].values() if s['initialized']])}"
        )
        print(f"   - Active validators: {len(stats['validators'])}")

        await liaison.shutdown()
        return

    except Exception as e:
        print(f"‚ùå Enhanced governance liaison test failed: {e}")
        import traceback

        traceback.print_exc()
        assert False, f"Enhanced governance liaison test failed: {e}"


async def test_integration():
    """Test integration of all enhanced components."""
    print("\nüß™ Testing Full Integration...")

    try:
        from grace.memory.enhanced_memory_bridge import EnhancedMemoryBridge
        from grace.mldl.enhanced_governance_liaison import EnhancedGovernanceLiaison

        # Create integrated system
        memory_bridge = EnhancedMemoryBridge(
            config={
                "vector_db": {"enabled": False},
                "quantum_storage": {
                    "enabled": True,
                    "storage_path": "/tmp/integration_test_storage",
                },
            }
        )

        governance_liaison = EnhancedGovernanceLiaison()

        # Initialize components
        memory_initialized = await memory_bridge.initialize()
        governance_initialized = await governance_liaison.initialize()

        if memory_initialized and governance_initialized:
            print("‚úÖ Integrated system initialized")
        else:
            print("‚ö†Ô∏è  Integrated system partially initialized")

        # Test end-to-end governance workflow
        governance_request = {
            "request_id": "integration_test_001",
            "task_type": "policy_approval",
            "data": {
                "policy_name": "AI Transparency Requirements",
                "policy_scope": "all_ai_systems",
                "implementation_timeline": "6_months",
            },
            "context": {"regulatory_pressure": "high", "stakeholder_support": "medium"},
            "technical_feasibility": 0.8,
            "legal_compliance": 0.9,
            "fairness_score": 0.85,
            "compliance_factors": {
                "transparency": 0.9,
                "accountability": 0.8,
                "fairness": 0.85,
            },
        }

        # Process through governance liaison
        governance_result = await governance_liaison.process_governance_request(
            governance_request
        )

        # Store decision in enhanced memory
        if governance_result["status"] == "success":
            decision_data = {
                "governance_result": governance_result,
                "decision_metadata": {
                    "processed_at": datetime.now().isoformat(),
                    "system_version": "enhanced_v1.0",
                },
            }

            store_result = await memory_bridge.store_enhanced(
                content=decision_data,
                content_type="governance_decision",
                metadata={"integration_test": True},
                trust_score=governance_result["consensus"]["consensus_confidence"],
                constitutional_score=governance_result["consensus"][
                    "consensus_cross_domain_score"
                ],
                sensitivity_score=0.7,
            )

            if store_result["status"] == "success":
                print(f"‚úÖ Integration test completed successfully")
                print(
                    f"   - Governance decision: {governance_result['governance_decision']['decision_type']}"
                )
                print(f"   - Decision stored: {store_result['content_id']}")

                # Test retrieval
                recall_result = await memory_bridge.recall_enhanced(
                    store_result["content_id"]
                )
                if recall_result.get("found"):
                    print("‚úÖ Decision successfully retrieved from enhanced memory")
            else:
                print("‚ö†Ô∏è  Decision storage had issues")

        # Clean up
        await memory_bridge.shutdown()
        await governance_liaison.shutdown()

        return

    except Exception as e:
        print(f"‚ùå Integration test failed: {e}")
        import traceback

        traceback.print_exc()
        assert False, f"Integration test failed: {e}"


async def main():
    """Run all enhanced Grace tests."""
    print("üöÄ Starting Enhanced Grace Infrastructure Tests")
    print("=" * 60)

    test_results = {}

    # Run all test suites
    test_suites = [
        ("Vector Database Integration", test_vector_database_integration),
        ("Quantum-Safe Storage", test_quantum_safe_storage),
        ("Enhanced Memory Bridge", test_enhanced_memory_bridge),
        ("Enhanced ML/DL Specialists", test_enhanced_ml_specialists),
        ("Cross-Domain Validators", test_cross_domain_validators),
        ("Enhanced Governance Liaison", test_enhanced_governance_liaison),
        ("Full Integration", test_integration),
    ]

    for test_name, test_func in test_suites:
        try:
            print(f"\n{'=' * 60}")
            print(f"Running: {test_name}")
            print("=" * 60)

            result = await test_func()
            test_results[test_name] = result

            if result:
                print(f"‚úÖ {test_name}: PASSED")
            else:
                print(f"‚ùå {test_name}: FAILED")

        except Exception as e:
            print(f"‚ùå {test_name}: ERROR - {e}")
            test_results[test_name] = False

    # Summary
    print("\n" + "=" * 60)
    print("üèÅ ENHANCED GRACE TEST SUMMARY")
    print("=" * 60)

    passed = sum(1 for result in test_results.values() if result)
    total = len(test_results)

    for test_name, result in test_results.items():
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"{test_name:30} {status}")

    print(f"\nüìä Overall Results: {passed}/{total} tests passed")

    if passed == total:
        print("üéâ ALL ENHANCED TESTS PASSED!")
        return
    else:
        print("‚ö†Ô∏è  Some tests failed or had issues")
        assert False, f"{total - passed} tests failed"


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Tests interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nüí• Test runner error: {e}")
        sys.exit(1)
