"""
Local AI Models for Grace

Run AI models locally - no external APIs needed!

Models:
- Whisper (Speech-to-Text) - Local
- Llama 2/3 (LLM) - Local via llama.cpp
- CodeLlama (Code generation) - Local
- Mistral (General purpose) - Local
- Sentence Transformers (Embeddings) - Local
- Piper/Coqui (Text-to-Speech) - Local

Grace runs entirely on your hardware!
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from pathlib import Path

logger = logging.getLogger(__name__)


@dataclass
class ModelConfig:
    """Configuration for local models"""
    whisper_model: str = "base"  # tiny, base, small, medium, large
    llm_model: str = "llama-2-7b"  # or mistral-7b, codellama-7b
    embedding_model: str = "all-MiniLM-L6-v2"
    tts_model: str = "piper"
    models_dir: str = "./models"
    use_gpu: bool = True


class LocalLLM:
    """
    Local LLM using llama.cpp or similar.
    
    No OpenAI/Anthropic API needed!
    Runs Llama 2, Mistral, CodeLlama, etc. locally.
    """
    
    def __init__(self, model_name: str = "llama-2-7b"):
        self.model_name = model_name
        self.model = None
        self.loaded = False
        
        logger.info(f"Local LLM initialized: {model_name}")
    
    async def load_model(self):
        """Load local LLM"""
        try:
            # Try llama-cpp-python
            from llama_cpp import Llama
            
            model_path = f"./models/{self.model_name}.gguf"
            
            logger.info(f"Loading {self.model_name}...")
            self.model = Llama(
                model_path=model_path,
                n_ctx=4096,  # Context window
                n_gpu_layers=-1  # Use all GPU layers
            )
            
            self.loaded = True
            logger.info(f"‚úÖ {self.model_name} loaded (running locally!)")
            
        except ImportError:
            logger.warning("llama-cpp-python not installed")
            logger.warning("Install: pip install llama-cpp-python")
            self.loaded = False
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            self.loaded = False
    
    async def generate(
        self,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.7,
        stream: bool = False
    ) -> str:
        """
        Generate text using local LLM.
        
        Completely local - no API calls!
        """
        if not self.loaded:
            await self.load_model()
        
        if not self.loaded:
            return "[Local LLM not available - install llama-cpp-python and download models]"
        
        try:
            # Generate
            result = self.model(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                stop=["User:", "\n\n\n"],
                echo=False
            )
            
            text = result['choices'][0]['text']
            
            logger.info(f"Generated {len(text)} characters locally")
            
            return text.strip()
            
        except Exception as e:
            logger.error(f"Generation failed: {e}")
            return ""


class LocalEmbeddings:
    """
    Local embedding model for semantic search.
    
    Uses Sentence Transformers - runs locally!
    """
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model = None
        self.loaded = False
    
    async def load_model(self):
        """Load embedding model"""
        try:
            from sentence_transformers import SentenceTransformer
            
            logger.info(f"Loading embedding model: {self.model_name}...")
            self.model = SentenceTransformer(self.model_name)
            self.loaded = True
            
            logger.info("‚úÖ Embedding model loaded (local)")
            
        except ImportError:
            logger.warning("sentence-transformers not installed")
            logger.warning("Install: pip install sentence-transformers")
        except Exception as e:
            logger.error(f"Failed to load embeddings: {e}")
    
    async def embed(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for texts"""
        if not self.loaded:
            await self.load_model()
        
        if not self.loaded:
            return [[0.0] * 384 for _ in texts]  # Fallback
        
        embeddings = self.model.encode(texts)
        return embeddings.tolist()


class LocalModelManager:
    """
    Manages all local AI models.
    
    Grace runs entirely on your hardware!
    """
    
    def __init__(self, config: Optional[ModelConfig] = None):
        self.config = config or ModelConfig()
        
        # Initialize models
        self.llm = LocalLLM(self.config.llm_model)
        self.embeddings = LocalEmbeddings(self.config.embedding_model)
        
        # Voice models initialized separately
        from grace.interface.voice_interface import get_voice_interface
        self.voice = get_voice_interface()
        
        logger.info("Local Model Manager initialized")
        logger.info(f"  LLM: {self.config.llm_model}")
        logger.info(f"  Embeddings: {self.config.embedding_model}")
        logger.info(f"  Voice: Whisper + Piper")
    
    async def initialize_all(self):
        """Load all models"""
        logger.info("Loading all local models...")
        
        # Load in parallel
        await asyncio.gather(
            self.llm.load_model(),
            self.embeddings.load_model(),
            self.voice.initialize()
        )
        
        logger.info("‚úÖ All local models loaded")
        logger.info("   Grace is running 100% locally!")
    
    async def generate_text(
        self,
        prompt: str,
        **kwargs
    ) -> str:
        """Generate text using local LLM"""
        return await self.llm.generate(prompt, **kwargs)
    
    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings"""
        return await self.embeddings.embed(texts)
    
    async def transcribe_audio(self, audio_data: bytes) -> str:
        """Transcribe audio"""
        return await self.voice.stt.transcribe_audio(audio_data)
    
    async def synthesize_speech(self, text: str) -> bytes:
        """Synthesize speech"""
        return await self.voice.tts.synthesize_speech(text)
    
    def get_model_status(self) -> Dict[str, Any]:
        """Get status of all models"""
        return {
            "llm": {
                "name": self.config.llm_model,
                "loaded": self.llm.loaded,
                "type": "local"
            },
            "embeddings": {
                "name": self.config.embedding_model,
                "loaded": self.embeddings.loaded,
                "type": "local"
            },
            "voice_stt": {
                "name": f"whisper-{self.config.whisper_model}",
                "loaded": self.voice.stt.loaded,
                "type": "local"
            },
            "voice_tts": {
                "name": self.config.tts_model,
                "loaded": self.voice.tts.loaded,
                "type": "local"
            },
            "all_local": True,
            "no_api_calls": True
        }


# Global model manager
_model_manager: Optional[LocalModelManager] = None


def get_local_models() -> LocalModelManager:
    """Get global local model manager"""
    global _model_manager
    if _model_manager is None:
        _model_manager = LocalModelManager()
    return _model_manager


if __name__ == "__main__":
    # Demo
    async def demo():
        print("ü§ñ Local AI Models Demo\n")
        
        models = LocalModelManager()
        
        print("Loading models (this may take a minute)...")
        await models.initialize_all()
        
        # Check status
        status = models.get_model_status()
        
        print("\nüìä Model Status:")
        for model_type, info in status.items():
            if isinstance(info, dict) and "name" in info:
                loaded_status = "‚úÖ" if info.get("loaded") else "‚ùå"
                print(f"  {loaded_status} {model_type}: {info['name']} ({info['type']})")
        
        print(f"\n‚úÖ All Local: {status['all_local']}")
        print(f"‚úÖ No API Calls: {status['no_api_calls']}")
        
        print("\nGrace is running 100% on your hardware!")
    
    asyncio.run(demo())
